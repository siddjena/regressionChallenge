---
title: "Regression & Interpretability Challenge"
subtitle: "Don't Trust Linear Models - The Perils of Non-Linearity"
format:
  html:
    toc: true
    number-sections: false
    theme: cosmo
execute:
  echo: false
  warning: false
  message: false
---

## Executive Summary

Linear regressions only tell the truth when the functional form matches reality. Using a stress–anxiety example, I contrasted models that rely on an imperfect proxy (`StressSurvey`) against models that use the true stress level. Every regression “fits” the data and produces highly significant coefficients, yet only the correctly specified model recovers the known causal relationship ($Anxiety = Stress + 0.1 \times Time$). The analysis surfaces three lessons: (1) monotonic proxies are not the same as linear controls, (2) statistical significance says nothing about interpretability, and (3) slicing the data into regimes with consistent functional forms is a practical guardrail.

## Data and Ground Truth

The synthetic data simulate anxiety measured via fMRI, true stress (via cortisol), a survey-based stress proxy, and minutes of social media use in the past 24 hours. By construction:

$$
Anxiety = Stress + 0.1 \times Time
$$

The `StressSurvey` proxy is monotonic but mildly curved relative to the true stress level, intentionally creating the interpretability trap.

```{python}
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import statsmodels.api as sm

plt.rcParams["figure.figsize"] = (7, 4)
sns.set_theme(style="whitegrid", context="talk")

observDF = pd.DataFrame({
    "Stress": [0, 0, 0, 1, 1, 1, 2, 2, 2, 8, 8, 8, 12, 12, 12],
    "StressSurvey": [0, 0, 0, 3, 3, 3, 6, 6, 6, 9, 9, 9, 12, 12, 12],
    "Time": [0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2.1, 2.2, 2.2, 2.2],
    "Anxiety": [0, 0.1, 0.1, 1.1, 1.1, 1.1, 2.2, 2.2, 2.2, 8.2, 8.2, 8.21, 12.22, 12.22, 12.22]
})

observDF
```

## Modeling Helpers

All regressions are ordinary least squares (OLS) fits computed with `statsmodels`. I also pre-compute utility functions so each question can focus on interpretation instead of mechanics.

```{python}
def fit_model(predictors, data=observDF):
    X = sm.add_constant(data[predictors])
    return sm.OLS(data["Anxiety"], X).fit()

model_stress_survey = fit_model(["StressSurvey"])
model_time = fit_model(["Time"])
model_stresssurvey_time = fit_model(["StressSurvey", "Time"])
model_stress_time = fit_model(["Stress", "Time"])

low_stress_df = observDF[observDF["Stress"] <= 6]
model_subset = fit_model(["StressSurvey", "Time"], data=low_stress_df)

def summarize_model(model):
    summary = pd.DataFrame({
        "term": model.params.index,
        "coef": model.params.values,
        "std_err": model.bse.values,
        "p_value": model.pvalues.values
    })
    return summary.round({"coef": 3, "std_err": 3, "p_value": 5})
```

## Question 1 — Bivariate Regression with `StressSurvey`

The OLS fit of `Anxiety ~ StressSurvey` produces a slope of **1.047** and an intercept of **−1.524**. Although the slope is close to the true stress coefficient (1.0), the intercept shift is substantial and signals model bias. The high $R^2$ (0.90) and tiny p-value for the slope (6.7e−8) create a false sense of reliability despite the biased functional form.

```{python}
summarize_model(model_stress_survey)
```

## Question 2 — Visualization of `StressSurvey` vs. Anxiety

The scatter plot (dependent variable on the vertical axis) shows the non-linear, piecewise structure of the proxy. The regression line slices through the cloud with a noticeable vertical offset, underestimating anxiety at low proxy values and overestimating it at high values. The curvature that is obvious in the plot is exactly what the linear regression cannot capture.

```{python}
fig, ax = plt.subplots()
sns.regplot(
    data=observDF,
    x="StressSurvey",
    y="Anxiety",
    ax=ax,
    scatter_kws={"s": 110, "color": "#6F2DBD"},
    line_kws={"color": "#D81B60", "linewidth": 2.5}
)
ax.set_xlabel("Stress Survey Response")
ax.set_ylabel("Anxiety (fMRI units)")
ax.set_title("Bivariate fit underestimates anxiety at low proxy values")
plt.tight_layout()
plt.show()
```

## Question 3 — Bivariate Regression with `Time`

Regressing `Anxiety ~ Time` alone yields a slope of **5.34** and an intercept of **−3.68**. The true causal coefficient of time spent on social media is only 0.1, so the bivariate fit exaggerates the effect by over 50× because it is forced to absorb stress variation that has been omitted. The model still reports a respectable $R^2$ (0.56) and a significant slope (p = 0.0013), illustrating how omitted variable bias converts a mild causal effect into an alarmist story.

```{python}
summarize_model(model_time)
```

## Question 4 — Visualization of `Time` vs. Anxiety

Only four unique time values exist in the data, so the regression line is effectively a steep staircase that chases group means. The plot highlights how a limited range of the independent variable causes overfitting: each cluster is vertically spread by stress differences, yet the regression interprets those differences as extra slope on time.

```{python}
fig, ax = plt.subplots()
sns.regplot(
    data=observDF,
    x="Time",
    y="Anxiety",
    ax=ax,
    scatter_kws={"s": 110, "color": "#0072B2"},
    line_kws={"color": "#FF7F0E", "linewidth": 2.5}
)
ax.set_xlabel("Minutes on Social Media (last 24 hours)")
ax.set_ylabel("Anxiety (fMRI units)")
ax.set_title("Sparse time values force the fit to overstate the slope")
plt.tight_layout()
plt.show()
```

## Question 5 — Multiple Regression with `StressSurvey` and `Time`

Adding `Time` alongside the proxy yields coefficients of **1.427** on `StressSurvey` and **−2.78** on `Time`, with both terms statistically significant. The model’s $R^2$ rises to 0.94, yet it now claims that holding the survey proxy constant, additional time online *reduces* anxiety. This is the classic interpretability failure: the regression has high explanatory power but reverses the sign of the time effect relative to the truth because the proxy distorts how stress is “controlled for.”

```{python}
summarize_model(model_stresssurvey_time)
```

## Question 6 — Multiple Regression with True `Stress` and `Time`

When stress is measured directly and included with time, OLS recovers the known coefficients exactly: intercept ≈ 0, stress coefficient = 1, and time coefficient = 0.1, with $R^2 = 1.0$. This regression is the reference point that every other specification should be compared against.

```{python}
summarize_model(model_stress_time)
```

## Question 7 — Model Comparison and Statistical Significance

Both multiple regressions present “convincing” statistics, yet only the model with true stress is interpretable. The table below juxtaposes coefficient magnitudes, signs, and $p$-values.

```{python}
comparison_df = pd.DataFrame([
    {
        "Model": "StressSurvey + Time",
        "R_squared": round(model_stresssurvey_time.rsquared, 3),
        "Stress-like term": round(model_stresssurvey_time.params["StressSurvey"], 3),
        "Time term": round(model_stresssurvey_time.params["Time"], 3),
        "p(Stress-like)": round(model_stresssurvey_time.pvalues["StressSurvey"], 5),
        "p(Time)": round(model_stresssurvey_time.pvalues["Time"], 5)
    },
    {
        "Model": "Stress + Time",
        "R_squared": round(model_stress_time.rsquared, 3),
        "Stress-like term": round(model_stress_time.params["Stress"], 3),
        "Time term": round(model_stress_time.params["Time"], 3),
        "p(Stress-like)": round(model_stress_time.pvalues["Stress"], 5),
        "p(Time)": round(model_stress_time.pvalues["Time"], 5)
    }
])

comparison_df
```

*Key takeaways:* (1) a 0.935 $R^2$ can still produce the wrong story; (2) “significant” does not mean “correct sign”; and (3) interpretation hinges on whether the control variable truly linearizes the relationship.

## Question 8 — Real-World Headlines

- **If the proxy-based model were published:** Popular press outlets would run headlines like *“Extra Hour on TikTok Lowers Anxiety, Study Finds”* because the estimated time coefficient is negative and significant. Parents seeking hope might embrace this narrative, even though it is entirely spurious.
- **If the true-stress model were published:** Headlines would instead read *“Stress Drives Anxiety, Social Media Adds a Small but Real Bump.”* This version resonates with the typical parent’s intuition that both stress and screen time are harmful. Social media companies would naturally prefer the first headline because it argues their platforms are protective. The same dataset therefore feeds opposite narratives depending on model specification.

## Question 9 — Subset Analysis to Avoid Misleading Significance

Following the “statistical regimes” tip, I restricted the sample to observations where `Stress ≤ 6`. In this low-to-moderate range the survey proxy is almost perfectly linear with true stress (exactly three survey points per stress unit), eliminating the curvature that caused coefficient flips. The resulting regression keeps both coefficients significant *and* close to the true relationship: `StressSurvey` enters with **0.333**, which maps back to a stress coefficient of 1 because `Stress = StressSurvey / 3` inside this regime, and `Time` retains the correct **0.1** effect with $R^2 = 1.0$. This exercise demonstrates how thoughtful subsetting can reveal whether a misleading coefficient arises from functional-form violations rather than sampling noise.

```{python}
summarize_model(model_subset)
```

**Bottom line:** Regression output is only as interpretable as the assumptions baked into the covariates. Visual diagnostics and regime-based splits are lightweight tools that keep “garbage can” regressions from producing garbage insights.

